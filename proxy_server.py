import json
import os
import sys
import globalVar
from typing import Optional, Union

import vertexai
from vertexai.generative_models import GenerativeModel
import vertexai.preview.generative_models as generative_models
from anthropic import AnthropicVertex
from dotenv import load_dotenv
from fastapi import FastAPI, Header, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from pydantic import BaseModel
from secrets import compare_digest

# åˆå§‹åŒ– FastAPI åº”ç”¨ç¨‹åº
app = FastAPI()

# è·å–ç¯å¢ƒå˜é‡ DOCKER_ENVï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®ï¼Œé»˜è®¤ä¸º False
is_docker = os.environ.get('DOCKER_ENV', 'False').lower() == 'true'


#åŠ è½½æ–‡ä»¶ç›®å½•
def get_base_path():
    if getattr(sys, 'frozen', False):
        # å¦‚æœæ˜¯æ‰“åŒ…åçš„å¯æ‰§è¡Œæ–‡ä»¶
        return os.path.dirname(sys.executable)
    else:
        # å¦‚æœæ˜¯ä»Pythonè¿è¡Œ
        return os.path.dirname(os.path.abspath(__file__))


# åŠ è½½ç¯å¢ƒå˜é‡
#env_path = os.path.join(get_base_path(), '.env')
#load_dotenv(env_path)

#os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = #os.path.join(get_base_path(), 'auth', 'auth.json')


jsondata = []
accountIndex = 0
accountName = ""
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(
    '/dev/shm', 'auth.json')
hostaddr = '0.0.0.0' if is_docker else os.environ.get('host', '127.0.0.1')

if len(os.environ.get('host', '127.0.0.1')) == 0:
    hostaddr = '127.0.0.1'

if len(os.environ.get('port', '5000')) == 0:
    lsnport = int(5000)
else:
    lsnport = int(os.environ.get('port', 5000))

if len(os.environ.get('region', 'us-east5')) == 0:
    region = 'us-east5'
else:
    region = os.environ.get('region','us-east5')

if len(os.environ.get('counter', '0')) == 0:
    timeToSwotch = int(0)
else:
    timeToSwotch = int(os.environ.get('counter', 0))

password = os.environ.get('password')
messageCount = 0

# VertexAI é…ç½®
vertex_client = AnthropicVertex(region=region)

def loadAccountData():
    start = 0
    global jsondata
    for index in range(globalVar.accountdata.count("{")):
        jsondata.append(globalVar.accountdata[globalVar.accountdata.index("{", start):globalVar.accountdata.index("}", start)+1])
        start = globalVar.accountdata.index("}",start)+1
    changeActiveAccount(0)
    

def changeActiveAccount(index):
    if index == len(jsondata):
        index = 0
    
    jsfile = json.dumps(jsondata[index]).replace('\\"', '"').replace('"{','{').replace('}"','}')
    with open('/dev/shm/auth.json', 'w') as f:
        f.write(jsfile)
    global accountIndex
    global accountName
    global vertex_client
    accountIndex = index
    starttemp = jsondata[index].index("project_id") + 11
    starttemp2 = jsondata[index].index("\"",starttemp) + 1
    accountName = jsondata[index][starttemp2:jsondata[index].index(",",starttemp)-1]
    vertex_client = AnthropicVertex(project_id=accountName, region=region)
    vertexai.init(project=accountName, location=region)
    print(f"\033[32mINFO\033[0m:     Logged in \"{accountName}\".Index: {accountIndex}")
    

loadAccountData()

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class MessageRequest(BaseModel):
    model: str
    stream: Optional[bool] = False
    # æ·»åŠ å…¶ä»–å¯èƒ½çš„å­—æ®µ


def vertex_model(original_model):
    # å®šä¹‰æ¨¡å‹åç§°æ˜ å°„
    mapping_file = os.path.join(get_base_path(), 'model_mapping.json')
    with open(mapping_file, 'r') as f:
        model_mapping = json.load(f)
    return model_mapping[original_model]


# æ¯”è¾ƒå¯†ç 
def check_auth(api_key: Optional[str]) -> bool:
    if not password:  # å¦‚æœå¯†ç æœªè®¾ç½®æˆ–ä¸ºç©ºå­—ç¬¦ä¸²
        return True
    return api_key and compare_digest(api_key, password)

@app.get("/")
async def ping():
    Main = 'Anthropic2Vertexä¿®æ”¹ç‰ˆ by zxcPandora'
    index_msg = "<!DOCTYPE html>\\n<html>\\n<head>\\n<meta charset=\"utf-8\">\\n<script>\\nfunction copyToClipboard(text) {\\n  var textarea = document.createElement(\"textarea\");\\n  textarea.textContent = text;\\n  textarea.style.position = \"fixed\";\\n  document.body.appendChild(textarea);\\n  textarea.select();\\n  try {\\n    return document.execCommand(\"copy\");\\n  } catch (ex) {\\n    console.warn(\"Copy to clipboard failed.\", ex);\\n    return false;\\n  } finally {\\n    document.body.removeChild(textarea);\\n  }\\n}\\nfunction copyLink(event) {\\n  event.preventDefault();\\n  const url = new URL(window.location.href);\\n  const link = url.protocol + '//' + url.host + '/v1';\\n  copyToClipboard(link);\\n  alert('é“¾æ¥å·²å¤åˆ¶: ' + link);\\n}\\n</script>\\n</head>\\n<body>\\n" + Main + "<br/><br/>å®Œå…¨å¼€æºã€å…è´¹ä¸”ç¦æ­¢å•†ç”¨<br/><br/>ç‚¹å‡»å¤åˆ¶åå‘ä»£ç†: <a href=\"v1\" onclick=\"copyLink(event)\">Copy Link</a><br/>å¤åˆ¶åå¡«å…¥ ä»£ç†æœåŠ¡å™¨ URL ä¸­å¹¶é€‰æ‹©ä½ åœ¨Vertexä¸­çš„å·²å¯ç”¨çš„claudeæ¨¡å‹ï¼ˆClaude API Keyä¸­éšä¾¿å¡«ç‚¹ä»€ä¹ˆï¼Œä½†ä¸èƒ½ä¸ºç©ºï¼‰<br/><br/>æ•™ç¨‹ä¸FAQ: <a href=\"https://rentry.org/zxcPandora_cloud_proxy\" target=\"FAQ\">Rentry</a> | <a href=\"https://github.com/TheValkyrja/Anthropic2Vertex\" target=\"FAQ\">Anthropic2VertexåŸä½œè€…ä»“åº“</a><br/><br/><br/>â—è­¦æƒ•ä»»ä½•é«˜é£é™©cookie/ä¼ªapi(25k cookie)è´­ä¹°æœåŠ¡ï¼Œä»¥åŠç ´åä¸­æ–‡AIå¼€æºå…±äº«ç¯å¢ƒå€’å–å…è´¹èµ„æºæŠ¹å»ç½²åçš„ç¾¤ç»„ï¼ˆğŸˆ²é»‘åå•ï¼šé…’é¦†å°äºŒã€AIæ–°æœåŠ¡ã€æµ…ç¡(é²‘é±¼)ã€èµ›åšå¥³å‹åˆ¶ä½œäºº(é’éºˆ/overloaded/ç§‘æ™®æ™“ç™¾ç”Ÿ)ğŸˆ²ï¼‰\\n</body>\\n</html>"
    return HTMLResponse(content = index_msg.replace("\\n", "\n").replace("\\", '').replace('\\"', '"'))

def translateResponseToSillytavernFormat(text,usage_metadata):
    responseData = {
        "candidates": [{
            "content": {
                "parts": [{
                    "text": text
                }],
                "role": "model"
            },
            "finishReason": "STOP",
            "index": 0
        }],
        "usageMetadata": {
            "promptTokenCount": usage_metadata.prompt_token_count,
            "candidatesTokenCount": usage_metadata.candidates_token_count,
            "totalTokenCount": usage_metadata.total_token_count
        }
    }
    return responseData

safety_settings = {
    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_NONE,
    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_NONE,
    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_NONE,
    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_NONE
}

@app.post("/v1beta/models/{requestModel}")
async def gemini_proxy(request: Request, requestModel: str ,key: str,alt:Union[str,None] = None):
    # å¯†ç éªŒè¯
    if not check_auth(key):
        raise HTTPException(status_code=401, detail="Unauthorized")

    if timeToSwotch != 0:
        global messageCount
        messageCount += 1
        if messageCount == timeToSwotch:
            changeActiveAccount(accountIndex+1)
            messageCount = 0
        
    data = await request.json()
    #print("Original request:")
    #print(data)

    #è¯·æ±‚è§£æ
    sendModel = requestModel.split(":")[0].replace("-latest", "-001")
    stream = alt or requestModel.split(":")[1] == "streamGenerateContent"
    contents = data.get('contents')
    generationConfig =  data.get('generationConfig')
    system_instruction = data.get('system_instruction')['parts']['text'] if data.get('system_instruction') else None

    #å¯¹æ¨¡å‹é…ç½®é€‰é¡¹è¿›è¡Œè½¬æ¢
    gemini_config = {}
    for key, value in generationConfig.items():
        if key == 'stopSequences':
            # ç¡®ä¿stopSequenceså€¼ä¸åŒ…å«ç©ºå­—ç¬¦ä¸²
            if any(not seq for seq in value):
                value = [seq for seq in value if seq]
            gemini_config["stop_sequences"] = value
        elif key == 'candidateCount':
            gemini_config["candidate_count"] = value
        elif key == 'maxOutputTokens':
            gemini_config["max_output_tokens"] = value
        elif key == 'topP':
            gemini_config["top_p"] = value
        elif key == 'topK':
            gemini_config["top_k"] = value
        elif key == 'responseMimeType':
            gemini_config["response_mime_type"] = value
        elif key == 'responseSchema':
            gemini_config["response_schema"] = value
        else:
            gemini_config[key] = value

    # æ¨¡å‹é…ç½®
    aiModel = GenerativeModel(model_name=sendModel, system_instruction=system_instruction)
    print(f"\033[32mINFO\033[0m:     Request Model: \"{sendModel}\"")

    try: 
        # å‘é€è¯·æ±‚åˆ° VertexAI
        # æ£€æŸ¥æ˜¯å¦ä¸ºæµå¼è¯·æ±‚
        if stream:
            def generate():
                for chunk in aiModel.generate_content(json.dumps(contents), generation_config=gemini_config, safety_settings=safety_settings, stream=True):
                    response = f"data: {json.dumps(translateResponseToSillytavernFormat(chunk.text,chunk.usage_metadata))}\n\n"
                    #print(f"{response}")
                    yield response
            
            return StreamingResponse(generate(),
                 media_type='text/event-stream',
                 headers={'X-Accel-Buffering': 'no'})
        else:
            response = aiModel.generate_content(json.dumps(contents), generation_config=gemini_config, safety_settings=safety_settings, stream=False)
            #print(response)
            return JSONResponse(content=translateResponseToSillytavernFormat(response.text,response.usage_metadata), status_code=200)
    except Exception as e:
        print(str(e))
        return JSONResponse(content={"error": str(e)}, status_code=500)

@app.post("/v1/messages")
async def proxy_request(request: Request, x_api_key: Optional[str] = Header(None)):
    # å¯†ç éªŒè¯
    if not check_auth(x_api_key):
        raise HTTPException(status_code=401, detail="Unauthorized")

    if timeToSwotch != 0:
        global messageCount
        messageCount += 1
        if messageCount == timeToSwotch:
            changeActiveAccount(accountIndex+1)
            messageCount = 0
    
    # è·å–åŸå§‹è¯·æ±‚æ•°æ®
    data = await request.json()

    #    print("Original request:")
    #    print(data)

    # å‡†å¤‡å‘é€åˆ° VertexAI çš„è¯·æ±‚
    try:
        # åˆ›å»ºä¸€ä¸ªæ–°çš„å­—å…¸æ¥å­˜å‚¨è¯·æ±‚å‚æ•°
        vertex_request = {}

        # éå†åŸå§‹è¯·æ±‚ä¸­çš„æ‰€æœ‰é”®å€¼å¯¹
        for key, value in data.items():
            if key == 'model':
                # å¯¹æ¨¡å‹åç§°è¿›è¡Œè½¬æ¢
                vertex_request[key] = vertex_model(value)
                print(f"\033[32mINFO\033[0m:     Request Model: \"{vertex_model(value)}\"")
            else:
                # ç›´æ¥å¤åˆ¶å…¶ä»–æ‰€æœ‰å‚æ•°
                vertex_request[key] = value

        # è¾“å‡ºå¤„ç†åçš„è¯·æ±‚


#        print("Processed request:")
#        print(json.dumps(vertex_request, indent=2))

# å‘é€è¯·æ±‚åˆ° VertexAI
# æ£€æŸ¥æ˜¯å¦ä¸ºæµå¼è¯·æ±‚
        if vertex_request.get('stream', False):

            def generate():
                yield 'event: ping\ndata: {"type": "ping"}\n\n'
                for chunk in vertex_client.messages.create(**vertex_request):
                    response = f"event: {chunk.type}\ndata: {json.dumps(chunk.model_dump())}\n\n"
                    #                    print(f"{response}")
                    yield response

            return StreamingResponse(generate(),
                                     media_type='text/event-stream',
                                     headers={'X-Accel-Buffering': 'no'})
        else:
            response = vertex_client.messages.create(**vertex_request)
            #            print(f"{response}")
            return JSONResponse(content=response.model_dump(), status_code=200)

    except Exception as e:
        return JSONResponse(content={"error": str(e)}, status_code=500)
